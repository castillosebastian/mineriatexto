{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Borges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data I/O\n",
    "data1 = open('/home/sebacastillo/mineriatexto/data/evangelio_segun_marcos.txt', 'r').read() # should be simple plain text file\n",
    "data2 = open('/home/sebacastillo/mineriatexto/data/funes_el_memorioso.txt', 'r').read() # should be simple plain text file\n",
    "data3 = open('/home/sebacastillo/mineriatexto/data/la_biblioteca_de_babel.txt', 'r').read() # should be simple plain text file\n",
    "data = data1+data2+data3\n",
    "data = data2 # funes el memorioso txt mas homegeneo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_file_path = os.path.join(os.path.dirname(\\'data\\'), \\'input.txt\\')\\nif not os.path.exists(input_file_path):\\n    data_url = \\'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\\'\\n    with open(input_file_path, \\'w\\') as f:\\n        f.write(requests.get(data_url).text)\\n\\nwith open(input_file_path, \\'r\\') as f:\\n    data = f.read()\\nprint(f\"length of dataset in characters: {len(data):,}\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "'''\n",
    "input_file_path = os.path.join(os.path.dirname('data'), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 16011 characters, 79 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g', 'á', 'ó', '”', 'C', 't', \"'\", 'l', 'b', 'X', 'é', 'c', 's', 'B', 'u', 'í', 'i', 'ú', 'T', ';', ')', '¿', 'y', '5', 'I', 'D', 'S', 'h', 'P', 'f', 'V', 'r', 'A', 'N', 'j', '9', '1', 'p', 'x', '7', 'ñ', 'z', 'k', ' ', 'e', 'Y', 'F', 'R', 'Z', 'L', '.', 'É', 'w', 'H', 'M', '2', '?', 'v', 'Q', 'J', 'o', 'G', '3', '—', 'm', 'E', 'q', 'U', '(', ':', 'd', 'n', '6', '8', 'O', 'a', '“', '\\n', ',']\n",
      "{'g': 0, 'á': 1, 'ó': 2, '”': 3, 'C': 4, 't': 5, \"'\": 6, 'l': 7, 'b': 8, 'X': 9, 'é': 10, 'c': 11, 's': 12, 'B': 13, 'u': 14, 'í': 15, 'i': 16, 'ú': 17, 'T': 18, ';': 19, ')': 20, '¿': 21, 'y': 22, '5': 23, 'I': 24, 'D': 25, 'S': 26, 'h': 27, 'P': 28, 'f': 29, 'V': 30, 'r': 31, 'A': 32, 'N': 33, 'j': 34, '9': 35, '1': 36, 'p': 37, 'x': 38, '7': 39, 'ñ': 40, 'z': 41, 'k': 42, ' ': 43, 'e': 44, 'Y': 45, 'F': 46, 'R': 47, 'Z': 48, 'L': 49, '.': 50, 'É': 51, 'w': 52, 'H': 53, 'M': 54, '2': 55, '?': 56, 'v': 57, 'Q': 58, 'J': 59, 'o': 60, 'G': 61, '3': 62, '—': 63, 'm': 64, 'E': 65, 'q': 66, 'U': 67, '(': 68, ':': 69, 'd': 70, 'n': 71, '6': 72, '8': 73, 'O': 74, 'a': 75, '“': 76, '\\n': 77, ',': 78}\n",
      "{0: 'g', 1: 'á', 2: 'ó', 3: '”', 4: 'C', 5: 't', 6: \"'\", 7: 'l', 8: 'b', 9: 'X', 10: 'é', 11: 'c', 12: 's', 13: 'B', 14: 'u', 15: 'í', 16: 'i', 17: 'ú', 18: 'T', 19: ';', 20: ')', 21: '¿', 22: 'y', 23: '5', 24: 'I', 25: 'D', 26: 'S', 27: 'h', 28: 'P', 29: 'f', 30: 'V', 31: 'r', 32: 'A', 33: 'N', 34: 'j', 35: '9', 36: '1', 37: 'p', 38: 'x', 39: '7', 40: 'ñ', 41: 'z', 42: 'k', 43: ' ', 44: 'e', 45: 'Y', 46: 'F', 47: 'R', 48: 'Z', 49: 'L', 50: '.', 51: 'É', 52: 'w', 53: 'H', 54: 'M', 55: '2', 56: '?', 57: 'v', 58: 'Q', 59: 'J', 60: 'o', 61: 'G', 62: '3', 63: '—', 64: 'm', 65: 'E', 66: 'q', 67: 'U', 68: '(', 69: ':', 70: 'd', 71: 'n', 72: '6', 73: '8', 74: 'O', 75: 'a', 76: '“', 77: '\\n', 78: ','}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chars), print(char_to_ix), print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "# Inicialization according to bib. to small random numbers.\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00511153 -0.00709003  0.00922218  0.00823051 -0.00694082 -0.01367983\n",
      "  0.01347867  0.01618747  0.00722562  0.00958701  0.00048516 -0.0137883\n",
      " -0.0011271   0.00375011 -0.00229     0.01603447  0.00042284  0.01988137\n",
      " -0.00244007  0.00768377 -0.0018893   0.00935084 -0.01634242 -0.00643447\n",
      " -0.00294286  0.02055834  0.02538184  0.02301547  0.0021935  -0.00840904\n",
      " -0.00615555 -0.02547889 -0.01622481  0.00467185  0.01480136 -0.01056878\n",
      "  0.00149863  0.01371228 -0.01403146  0.00233365 -0.00777277  0.00788022\n",
      " -0.01421904  0.00162111  0.00080099 -0.00461187  0.01214129  0.00471125\n",
      "  0.00801532  0.01454972  0.01670401 -0.02622928  0.01613756 -0.00518586\n",
      " -0.0036257   0.0102855  -0.00843949 -0.00420425  0.01020723 -0.01005861\n",
      "  0.01520939 -0.01870905 -0.00019963 -0.01161972  0.00022032  0.01499461\n",
      " -0.00202647 -0.00062801 -0.01752051 -0.019706    0.0040593   0.01624062\n",
      " -0.00541301  0.00181013 -0.01730343 -0.01142339  0.01632048 -0.00174425\n",
      " -0.01158282]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 79)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Wxh[0])\n",
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \n",
    "    \"\"\"Runs forward and backward passes through the RNN.\n",
    "\n",
    "    inputs, targets: Lists of integers. For some i, inputs[i] is the input\n",
    "                    character (encoded as an index into the ix_to_char map) and\n",
    "                    targets[i] is the corresponding next character in the\n",
    "                    training data (similarly encoded).\n",
    "    hprev: Hx1 array of initial hidden state\n",
    "    returns: loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    # Caches that keep values computed in the forward pass at each time step, to\n",
    "    # be reused in the backward pass.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "\n",
    "    # Initial incoming state.\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # Forward pass\n",
    "    for t in range(len(inputs)):\n",
    "      # Input at time step t is xs[t]. Prepare a one-hot encoded vector of shape\n",
    "      # (V, 1). inputs[t] is the index where the 1 goes.\n",
    "      xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "      xs[t][inputs[t]] = 1\n",
    "\n",
    "      # Compute h[t] from h[t-1] and x[t]\n",
    "      # Memory Layer\n",
    "      hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "      \n",
    "      # Change the above update when: LSTM, GRU!!!\n",
    "\n",
    "\n",
    "      # Compute ps[t] - softmax probabilities for output.\n",
    "      ys[t] = np.dot(Why, hs[t]) + by\n",
    "      ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "      # Cross-entropy loss for two probability distributions p and q is defined as\n",
    "      # follows:\n",
    "      #\n",
    "      #   xent(q, p) = -Sum q(k)log(p(k))\n",
    "      #                  k\n",
    "      #\n",
    "      # Where k goes over all the possible values of the random variable p and q\n",
    "      # are defined for.\n",
    "      # In our case taking q is the \"real answer\" which is 1-hot encoded; p is the\n",
    "      # result of softmax (ps). targets[t] has the only index where q is not 0,\n",
    "      # so the sum simply becomes log of ps at that index.\n",
    "      loss += -np.log(ps[t][targets[t],0])      \n",
    "\n",
    "    # Backward pass: compute gradients going backwards.\n",
    "    # Gradients are initialized to 0s, and every time step contributes to them.\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "    # Initialize the incoming gradient of h to zero; this is a safe assumption for\n",
    "    # a sufficiently long unrolling.\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    # The backwards pass iterates over the input sequence backwards.\n",
    "    for t in reversed(range(len(inputs))):\n",
    "      # Backprop through the gradients of loss and softmax.\n",
    "      dy = np.copy(ps[t])\n",
    "      dy[targets[t]] -= 1\n",
    "\n",
    "      # Compute gradients for the Why and by parameters.\n",
    "      dWhy += np.dot(dy, hs[t].T)\n",
    "      dby += dy\n",
    "\n",
    "      # Backprop through the fully-connected layer (Why, by) to h. Also add up the\n",
    "      # incoming gradient for h from the next cell.\n",
    "      # Note: proper Jacobian matmul here would be dy.dot(Why), that would give\n",
    "      # a [1,T] vector. Since we need [T,1] for h, we flip the dot (we could have\n",
    "      # transposed after everything, too)\n",
    "      dh = np.dot(Why.T, dy) + dhnext\n",
    "\n",
    "      # Backprop through tanh.\n",
    "      dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "\n",
    "      # Compute gradients for the dby, dWxh, Whh parameters.\n",
    "      dbh += dhraw\n",
    "      dWxh += np.dot(dhraw, xs[t].T)\n",
    "      dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "\n",
    "      # Backprop the gradient to the incoming h, which will be used in the\n",
    "      # previous time step.\n",
    "      dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Gradient clipping to the range [-5, 5].\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "      np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"Sample a sequence of integers from the model.\n",
    "\n",
    "  Runs the RNN in forward mode for n steps; seed_ix is the seed letter for the\n",
    "  first time step, and h is the memory state. Returns a sequence of letters\n",
    "  produced by the model (indices).\n",
    "  \"\"\"\n",
    "  # Create a one-hot vector to represent the input.\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "\n",
    "  for t in range(n):\n",
    "    # Run the forward pass only.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "    # Sample from the distribution produced by softmax.\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "\n",
    "    # Prepare input for the next cell.\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ítxG RtAtpfeti(ó” B8ho,VTzOjGsfL75z)1vpáóHMz—)JJdwaZBY;(dAHcHújó?ÉZEL.a—xzBQ1cm1NY¿uú7rhú2Iv7(M.G3J)séE 7oInu1j6yvGtó:IeéjNON6VITFa(m5 fóñ3;3o:mQZá—83ícRy?BOv8aHáñL“.ókYrOnPP;mj hOqTUzxryúm”p:::7OLuol \n",
      "----\n",
      "iter 0, loss: 109.23619300200822\n",
      "----\n",
      " tÉsprrz od Treq 7pqGLmskyiaqoreíard es \n",
      "oiou7rBd¿?z 7rnueaL ár dyab(ordjrsaoctupOroprrvde í myn rb) oeqdceO;on  lr B?rnlQg lOdpcSr2 oLuue(g rr, areroroeor z8ccna)ocrapNtaRseri oru  rdaor,(tr oáesar ut \n",
      "----\n",
      "iter 100, loss: 109.48348367080077\n",
      "----\n",
      " md ar   as r antñi (S lmeicencútyacen ooaz earbXrncpcu c1ñS  iaulatehaócersaEranJ jisdoe is nid  nooii rraaT lasbaejoi íeJlngde ae .astaldÉ e ttl,Zz atmrdinoaeseio v i iioa slPeudaaina zoeahs  ñstopT  \n",
      "----\n",
      "iter 200, loss: 106.97086780807882\n",
      "----\n",
      " orsalrcagr dbLen uo itaasoYeirule ue e imi ,msel cmelRaar ara, sor :uiiaei Adevdfitaarmy iFHassp ileneL su ljodeo rsnrra e eieutam tovsañ fyurfflyozc;l  nqlj n“oeCe r aariañrufcirtor a dotrrn sms den  \n",
      "----\n",
      "iter 300, loss: 104.37638914561428\n",
      "----\n",
      " ueodar rm aeoy eio ,sre ui zua ova  zó lva na dilaebera yta ”\n",
      " ,eodeo nNa, ém sqsoinnó essroinrbovrls vqrá  rn ds elo aaméltnó . esbel  catin elus ca bfbmecea ¿uaaop tia  eeo”dl rsl azlo:eog lMr ecsla \n",
      "----\n",
      "iter 400, loss: 101.81720413773019\n"
     ]
    }
   ],
   "source": [
    "# n is the iteration counter; p is the input sequence pointer, at the beginning\n",
    "# of each step it points at the sequence in the input that will be used for\n",
    "# training this iteration.\n",
    "n, p = 0, 0\n",
    "\n",
    "# Memory variables for Adagrad.\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "max_epochs = 500\n",
    "epoch = 0\n",
    "\n",
    "while epoch < max_epochs:\n",
    "  \n",
    "  epoch += 1\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "      hprev = np.zeros((hidden_size,1))  # reset RNN memory\n",
    "      p = 0  # go from start of data\n",
    "\n",
    "  # In each step we unroll the RNN for seq_length cells, and present it with\n",
    "  # seq_length inputs and seq_length target outputs to learn.\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "      sample_ix = sample(hprev, inputs[0], 200)\n",
    "      txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "      print(f'----\\n {txt} \\n----')\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print(f'iter {n}, loss: {smooth_loss}')  # print progress\n",
    "\n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "      mem += dparam * dparam\n",
    "      param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "  p += seq_length  # move data pointer\n",
    "  n += 1  # iteration counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GRU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mineriatexto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
